# This file should contain all the record creation needed to seed the database with its default values.
# The data can then be loaded with the rails db:seed command (or created alongside the database with db:setup).
#
# Examples:
#
#   movies = Movie.create([{ name: 'Star Wars' }, { name: 'Lord of the Rings' }])
#   Character.create(name: 'Luke', movie: movies.first)
user = User.create! :username => 'tom', :email => 'thomas@thomashastings.com', :password => 'changeme', :password_confirmation => 'changeme'
Post.create( created_at:"2019-01-13T10:41:25.689Z",updated_at:"2019-01-13T21:57:03.281287503Z",title:"My Top 5 Research Tools for Computer Science", tags:"",author:"Tom", content:"\u003cp\u003eI've spent the last four years in graduate school and I've learned about some interesting tools for research and wanted to share. Many of these would have come in handy for my undergrad program as well.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"https://www.zotero.org/\"\u003eZotero\u003c/a\u003e\u003cbr\u003eZotero provides an easy way to manage bibliographies and includes easy export for Bibtex. It really is a great research assistant.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.overleaf.com/\"\u003eOverleaf\u003c/a\u003e\u003cbr\u003eOverleaf is a great tool for working with LaTex. It provides a web based editor for individuals or teams to work on documents. Overleaf also provides export capabilities to GitHub for team collaboration.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/\"\u003eGitHub\u003c/a\u003e\u003cbr\u003eGitHub provides Git repositories for team collaboration. Microsoft just announced that GitHub will allow unlimited private repos for free.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://education.github.com/pack\"\u003eStudent Developer Pack\u003c/a\u003e\u003cbr\u003eThe student developer pack from GitHub provides tons of goodies from companies like Amazon Web Services, Data Dog, Digital Ocean and others.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://scholar.google.com/\"\u003eGoogle Scholar\u003c/a\u003e\u003cbr\u003eLast but definitely not least… Google Scholar provides great resources for researchers. Everything from research papers to H-index and conference rankings. Google Scholar has it all.\u003c/li\u003e\u003c/ol\u003e")
Post.create( created_at:"2019-01-12T10:41:25.689Z",updated_at:"2019-01-13T21:52:13.648596351Z",title:"New Year with JAMstack", tags:"",author:"Tom", content:"\u003cp\u003eThe \u003ca href=\"https://jamstack.org/\"\u003eJAMstack\u003c/a\u003e is a relativity new concept in web development and it caught my attention because of the simplicity and speed at which pages load. The stack consists of Javascript, APIs, and Markup. Friends of mine had started coding in Go earlier in 2018 and I saw an opportunity to jump in head first into the new stack with Go.\u003c/p\u003e\u003cp\u003eI began working on the Go web service that serves the blog posts about three weeks ago. I developed a simple service that provides a headless \u003ca href=\"https://github.com/tghastings/blog\"\u003eRESTful API\u003c/a\u003e to create, read, update, and delete blog posts. I also wrote a user authentication package which uses bcrypt for password hashing and JSON Web Tokens for API authentication. This was also my first experience writing \u003ca href=\"https://tghastings.github.io/blog/\"\u003eAPI documentation\u003c/a\u003e using Swagger. The API is hosted on \u003ca href=\"http://www.digitalocean.com\"\u003eDigital Ocean\u003c/a\u003e and served using \u003ca href=\"http://www.cloudflare.com\"\u003eCloudflare\u003c/a\u003e. Once the API was serving JSON I had to tweak my front-end to request and receive the JSON coming from the API and parse it.\u003c/p\u003e\u003cp\u003eThe front-end uses basic HTML and CSS along with \u003ca href=\"https://vuejs.org/\"\u003evue.js\u003c/a\u003e and vanilla JavaScript. This was my first time developing with vue.js and because I needed something simple I did not take advantage of all the features vue provides. The front-end is hosted for free with SSL on \u003ca href=\"http://pages.github.com\"\u003eGitHub Pages\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf you are looking for an open source project to contribute to please consider contributing to this. The API and front-end source code are all open source and you can see the roadmap for the API in the README on GitHub. You can view the API \u003ca href=\"https://github.com/tghastings/blog\"\u003ecode\u003c/a\u003e and \u003ca href=\"https://tghastings.github.io/blog/\"\u003edocumentation\u003c/a\u003e. The \u003ca href=\"https://github.com/tghastings/thomashastings.com\"\u003eHTML and JavaScript\u003c/a\u003e for my site is also available on GitHub.\u003c/p\u003e")
Post.create( created_at:"2018-09-18T10:41:25.689Z",updated_at:"2019-01-13T06:08:15.582108898Z",title:"Software Supply Chain Open Source Issues. Part 1.", tags:"",author:"Tom", content:" \u003cp\u003eWith the rise of languages that provide package management tools, developers and software engineers are spending more time integrating than coding. There are a lot of great open source projects around the internet with many that are free to use. These projects vary greatly in terms of what they provide. Some provide advanced features such as machine learning frameworks, user authentication, data modeling, and time-series analysis. Others provide simple functionality such as padding text to the left or right in a webpage. Open source packages save developers hundreds of hours by providing functionality that the developer does not have to write herself. The developer can take the open source project and the code, modify it; if needed, and integrate the new functionality into her application. \u003c/p\u003e \u003cp\u003eA convenient way to include open source projects in software projects is through package managers. The process for including an open source project, in most cases, is as easy as defining the project name and version in a text file. The developer can then run an install command to bring in the open source code. \u003cem\u003eThe code is now part of the developer’s software\u003c/em\u003e. Many developers then take advantage of the new features which they just installed and continue on with the project.\u003c/p\u003e \u003cp\u003e\u003cstrong\u003eThe key statement is that the open source project code is now part of the developer’s code base for her project\u003c/strong\u003e. Not only is the code part of the code base but the code will, in most cases, be pushed to production. This should raise a series of questions within the organization. What dependencies did the open source project bring in? Do those dependencies have known vulnerabilities? What other features or functions does the open source project provide? Do those features or functions increase the attack surface for hackers by providing additional attack vectors? Or worse, does the project contain backdoors or other avenues for malicious authors / contributors to gain access?\u003c/p\u003e \u003cp\u003eThese are all risks associated with using open source projects. What data points could alleviate some of the risk to the organization when researching projects? Are projects with large communities more secure? Are projects with a certain number of downloads more secure? Do the number of collaborators make a difference? How about the number of commits? The number of pull requests? What metrics can be used to highlight and differentiate one project as potentially more secure than another? The total numbers of lines of code? I’ll attempt to answer these questions as I continue writing other parts of this series. \u003c/p\u003e")
Post.create( created_at:"2018-07-24T10:41:25.689Z",updated_at:"2019-01-13T06:07:01.976464223Z",title:"User Interaction Metrics for Hybrid Mobile Applications", tags:"",author:"Tom", content:"\u003cp\u003eUnderstanding user behavior and interactions in mobile applications is critical for developers to understand where to spend limited resources when adding, updating, and testing features but current tools do not do a good job of providing actionable insights. User behavior insights can provide value to the developer when it’s time to code and implement new features. Google Analytics and New Relic provide user insights but they fall short when it comes to identifying user interactions and behaviors as it pertains to individual features of mobile applications. We have developed a framework with middleware that provides user interaction insights, using time-series analysis, to hybrid mobile applications along with an empirical study to showcase the value of the framework.\u003c/p\u003e\u003cp\u003eFull Paper: \u003ca href=\"https://csce.ucmss.com/cr/books/2018/LFS/CSREA2018/SER4282.pdf\" target=\"_blank\"\u003eUser Interaction Metrics for Hybrid Mobile Applications\u003c/a\u003e \u003cbr\u003e ISBN: 1-60132-489-8, CSREA Press © Pages 30-35\u003c/p\u003e")
Post.create( created_at:"2018-04-30T10:41:25.689Z",updated_at:"2019-01-13T06:03:49.865779085Z",title:"What's In Your Container?", tags:"",author:"Tom", content:"\u003cp\u003eI'm excited to be speaking at JFrog's swampUP conference in May.\u003c/p\u003e\u003cp\u003eI'll be speaking on using Xray and Artifactory to produce secure containers. Avoiding known security vulnerability in prod, providing the US Gov with a complete Bill or Materials and ensuring compliance with copyright laws does not need to be scary. A brief case study in how to use JFrog products to support missions and developers around the globe.\u003c/p\u003e\u003cp\u003e\u003cimg src=\"//res.cloudinary.com/innopar/image/upload/v1522428036/nlpewrsp9n5saly4m4lp.png\" alt=\"TomHastingsSwampUpSpeaker\"\u003e\u003c/p\u003e")
Post.create( created_at:"2017-12-20T10:41:25.689Z",updated_at:"2019-01-13T06:02:26.921842036Z",title:"Ad-hoc Ansible Commands", tags:"",author:"Tom", content:"\u003cp\u003eSometimes I like to stash commands that I use regularly. Below is a snippet of code that I find helpful from time to time.\u003c/p\u003e\u003cp\u003e\u003ccode\u003eansible {{comma separated host list}} -m shell -a 'shell command goes here' --become\u003c/code\u003e\u003c/p\u003e")
Post.create( created_at:"2017-12-18T10:41:25.689Z",updated_at:"2019-01-13T06:01:11.469220845Z",title:"Docker Catch Sigterm", tags:"",author:"Tom", content:"\u003cp\u003eSometimes I like to stash commands that I use regularly. Below is a snippet of code that I find helpful from time to time.\u003c/p\u003e\u003ca href=\"https://gist.github.com/tghastings/54668646fbb6e12ee8f34f9cafe9dbde#file-docker-sigterm\"\u003e View Gist Here\u003c/a\u003e")
Post.create( created_at:"2017-05-10T10:41:25.689Z",updated_at:"2019-01-13T05:55:30.886840923Z",title:"Cost of Securing IEEE 802.11s Mesh Networks Using CJDNS", tags:"",author:"Tom", content:"\u003cp\u003e\u003cstrong\u003eAbstract\u003c/strong\u003e - The Internet is weak, it is broken, and we are not doing anything to fix it. The Internet can be affected by natural disasters, wars, governments, and surveillance. It is running out of address space and the internet service providers are not incentivized to fix it. Mesh networks, using the IEEE standard 802.11s, may one day provide a more robust and resilient infrastructure. Although mesh networking is not a new idea or a new concept, wireless mesh networking is stripping previous barriers to entry. IEEE 802.11s makes mesh networks a reality for users who otherwise would never have been able to setup such a distributed network. Applications like cjdns are making it easier than ever to create secure wireless mesh network among communities. This paper will look at the system costs associated with using cjdns. How much performance are we willing to sacrifice for ease of use and security?\u003c/p\u003e\u003cp\u003eFull Paper: \u003ca href=\"https://res.cloudinary.com/innopar/image/upload/v1494441393/Hastings-Cost_of_Securing_IEEE_802_11s_Mesh_Networks_with_CJDNS_gghd3e.pdf\" target=\"_blank\"\u003eCost of Securing IEEE 802.11s Mesh Networks Using CJDNS\u003c/a\u003e\u003c/p\u003e")
Post.create( created_at:"2017-03-18T10:41:25.689Z",updated_at:"2019-01-13T05:52:08.575821977Z",title:"SailsJS Error on Install: npm ERR! enoent ENOENT", tags:"",author:"Tom", content:"\u003ch3\u003eError: npm ERR! enoent ENOENT: no such file or directory while installing module sails\u003c/h3\u003e\u003cp\u003eIf I run 'sails new app' everything installs fine. If I go into the new app \u003ccode\u003ecd app\u003c/code\u003e and run \u003ccode\u003enpm install\u003c/code\u003e it runs fine. If I wipe out the 'node_modules' directory and run \u003ccode\u003enpm install\u003c/code\u003e I get errors:\u003c/p\u003e \u003cp\u003e\u003ccode\u003enpm ERR! enoent ENOENT: no such file or directory, chmod '/node_modules/sails/node_modules/anchor/node_modules/geojsonhint/node_modules/jsonlint-lines/node_modules/nomnom/node_modules/chalk/node_modules/strip-ansi/cli.js'\u003c/code\u003e\u003c/p\u003e\u003ch3\u003eTemporary Fix Until Pull Request Gets Merged or Version 1.0 is Released\u003c/h3\u003e\u003cp\u003eInside of the sails application go to the 'package.json' and replace: \u003cbr\u003e\u003ccode\u003e'sails': '~0.12.13',\u003c/code\u003ewith: \u003cbr\u003e\u003ccode\u003e'sails': 'github:tghastings/sails#hastings-fix',\u003c/code\u003e\u003c/p\u003e\u003cp\u003eAnd then run: \u003cbr\u003e\u003ccode\u003erm -rf node_modules\u003c/code\u003e \u003cbr\u003e\u003ccode\u003enpm cache clean\u003c/code\u003e \u003cbr\u003e\u003ccode\u003enpm install\u003c/code\u003e\u003c/p\u003e\u003ch3\u003ePull Request\u003c/h3\u003e\u003cp\u003eI've opened a pull request with the anchor project on GitHub\u003c/a\u003e. I had to add the 'geojsonhint' dependency back into the package.json. \u003c/p\u003e")
Post.create( created_at:"2017-03-12T10:41:25.689Z",updated_at:"2019-01-13T05:49:06.63805264Z",title:"DisplayLink Video - Ubuntu 16.10 - 1 FPS Issue: Fixed", tags:"",author:"Tom", content:"\u003cp\u003eI ran into an issue using my Dell USB 3.0 dock when I upgraded to Ubuntu 16.10 on my Dell XPS-13. I was getting ~1 FPS using the DisplayLink driver. I ended up having to turn off VSync. Hopefully DisplayLink releases an update soon to fix this.\u003c/p\u003e\u003cp\u003e\u003cem\u003e/etc/X11/xorg.conf.d/20-intel.conf:\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003ccode\u003eSection 'Device' Identifier 'Intel Graphics' \u003cbr\u003eDriver 'intel' \u003cbr\u003eOption 'VSync' 'false' \u003cbr\u003eEndSection\u003c/code\u003e\u003c/p\u003e")
Post.create( created_at:"2016-12-29T23:41:25.689Z",updated_at:"2019-01-13T05:45:32.76939718Z",title:"What I learned developing real-time web applications", tags:"",author:"Tom", content:"\u003ch3\u003eOverview\u003c/h3\u003e\u003cp\u003eOver the last two years I’ve had the privilege of working on and developing a web application which displays real-time hardware and software information. The application utilizes middleware to communicate with drivers on different hardware platforms. When changes in values are detected from the drivers, the middleware software fires a server-sent event. The server-sent events are aggregated and then sent from the aggregation server up to the front-end client. From there, Javascript is used to parse the messages and append updates to the graphical user interface. On average there are 29 messages sent every second. The average message size is 3KBs. Within a minute the system generates roughly 5MBs worth of messages.\u003c/p\u003e\u003ch3\u003eDocument Object Model (DOM) Manipulation\u003c/h3\u003e\u003cp\u003eThe system, on average, sends 29 messages every second to the front-end. 29 DOM updates every second will bring any web application to it’s knees if the JavaScript isn’t implemented correctly. Below are 5 things that I learned in regards to updating and manipulating the DOM.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eEach Javascript function that updates or manipulates the DOM creates a reflow and redraw which is work for the browser (the less work the browser has to do the better). \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIt’s optimal to update the DOM all at once with one function, if you can. This limits the amount of work the browser has to do with reflow because the browser will wait for the function to finish and only execute one reflow and repaint.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBe specific about the element that you want to edit in the HTML. This is where good design comes into play. Wrap HTML values in ‘\u003cspan\u003e\u003c/span\u003e’ for values that will change based on events. For example:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e\u0026lt;h1\u0026gt;Welcome, \u0026lt;span id=”welcome-user-name”\u0026gt;user\u0026lt;/span\u0026gt;\u0026lt;/h1\u0026gt;\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eDoing it this way will allow you to target the span by using the id to update the element, instead of having to update the entire header element.  \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnly update DOM elements that the user can see. The application I worked on was a single page website and we had div layers hidden for individual pages. There is no reason to update hidden elements in real-time. We worked around this by calling a ‘GET’ on the RESTful endpoints when a user clicked on a new page, that was hidden previously, to retrieve the most current information and then updated the content based on events after the initial pull.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo NOT rely heavily on the front-end for logical reasoning. The less work the browser has to do the better. All data should be parsed and managed in the model and then forwarded to the view. For example, when dealing with hardware, hardware can be in a bad state. Say a fan on a server is bad. This is something a user will want to know but it shouldn’t be the front-end’s responsibility to compare values to figure out if something is out of range. That should have been worked out by the time the data is sent to the view. The view just needs to know that if a bad state is present and handle it accordingly.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eEvent Source\u003c/h3\u003e\u003cp\u003eThe event source caused some issues early on with the application. Below are 5 lessons that I learned.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eGoogle Chrome limits the number of open sockets to 6. Each event source counts as an open socket. Each refresh also counts as an open socket. If you attempt to have 6 event sources open and refresh you’ll run into a “waiting for available socket” error. The fewer event sources open at one time, the better.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNew windows in Google Chrome don’t automatically mean that the new window is running under a new process. This means that new windows with event sources will still be limited to the 6 socket sum across both windows. To work around this we register new windows with the current window and pass the server-sent events from the main window to the new window without opening additional event sources.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe browser needs to close and open a new socket occasionally. The interval for restarting will depend on the amount of data coming through the pipe. We observed that a browser left open for over 6 hours became sluggish. The issue had all of the symptoms of a classic memory leak. Using Chrome’s developer tools we noticed that the event source, for every message, continued to grow in size. In order for the browser to garbage collect on the event source it needs to be closed. Opening a new event source and closing the old one fixed this issue.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eServer-side applications can break the front-end client. We had an issue where a browser would fail to keep up with the messages coming from the server and gradually become sluggish to the point of hitting the “aw snap” page in Chrome. In order to overcome this challenge we made a queue, for each client in a thread, on the backend that aggregates the server-sent events and passes those on to the listener. If the queue to the client’s listener begins to backup the backend will close the connection and attempt to establish a new connection. This prevents one client from causing a domino effect on the backend and causing the other clients to crash.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn this application, because we had so many messages, we needed a way to filter messages that we cared about at any given time. When we first wrote the application we listened to every message, regardless of relevance. As the project matured we realized that we needed a way to filter messages. We were able to do this by passing in variables to the backend with attributes that described what messages we cared about. We ended up with two event sources running in parallel on the client. The first event source listens for critical status messages while the other listens for applicable data based on the page the user is viewing.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eConclusion\u003c/h3\u003e\u003cp\u003eThis was the most challenging and most fun project that I’ve worked on to date. I’m excited for the future of web applications and all of the utility that new web languages bring to the craft.")
Post.create( created_at:"2016-12-10T23:41:25.689Z",updated_at:"2019-01-13T05:38:03.678809034Z",title:"Welcome", tags:"",author:"Tom", content:"\u003cp\u003eWelcome. I just updated my blog and moved it on to Heroku. Stay tuned for new updates.\u003c/p\u003e")